import os
import json
import uuid
import subprocess
import shutil
from ..config.settings import Config

class MiningService:
    @staticmethod
    def run_miner(input_file_path, job_id=None, config=None):
        """
        Runs the subgraph miner on the given input file.
        Returns the parsed JSON results and file paths.
        """
        if job_id is None:
            job_id = str(uuid.uuid4())
        
        shared_job_dir = "/shared/output/{}".format(job_id)
        os.makedirs(shared_job_dir, exist_ok=True)
        
        # Clean plots directory to prevent old results from mixing with new ones
        plots_cluster_dir = "/app/plots/cluster"
        if os.path.exists(plots_cluster_dir):
            shutil.rmtree(plots_cluster_dir)
        os.makedirs(plots_cluster_dir, exist_ok=True)
        
        out_filename = str(uuid.uuid4()) + '.pkl'
        out_path = os.path.join(Config.RESULTS_FOLDER, out_filename)
        json_path = os.path.join(Config.RESULTS_FOLDER, out_filename.replace('.pkl', '.json'))
        
        # Instance files generated by decoder.py
        instance_json_path = os.path.join(Config.RESULTS_FOLDER, out_filename.replace('.pkl', '_all_instances.json'))
        instance_pkl_path = os.path.join(Config.RESULTS_FOLDER, out_filename.replace('.pkl', '_all_instances.pkl'))

        try:
            # Build command dynamically
            cmd = [
                "python3", "-m", "subgraph_mining.decoder",
                "--dataset={}".format(input_file_path),
                "--out_path={}".format(out_path)
            ]

            if config:
                if config.get('n_trials'):
                    cmd.append("--n_trials={}".format(config['n_trials']))
                
                if config.get('min_pattern_size'):
                    cmd.append("--min_pattern_size={}".format(config['min_pattern_size']))
                    
                if config.get('max_pattern_size'):
                    cmd.append("--max_pattern_size={}".format(config['max_pattern_size']))

                if config.get('min_neighborhood_size'):
                    cmd.append("--min_neighborhood_size={}".format(config['min_neighborhood_size']))
                    
                if config.get('max_neighborhood_size'):
                    cmd.append("--max_neighborhood_size={}".format(config['max_neighborhood_size']))
                    
                if config.get('n_neighborhoods'):
                    cmd.append("--n_neighborhoods={}".format(config['n_neighborhoods']))
                
                if config.get('graph_type'):
                    cmd.append("--graph_type={}".format(config['graph_type']))
                    
                if config.get('radius'):
                    cmd.append("--radius={}".format(config['radius']))
                    
                if config.get('search_strategy'):
                    cmd.append("--search_strategy={}".format(config['search_strategy']))
                    
                if config.get('sample_method'):
                    cmd.append("--sample_method={}".format(config['sample_method']))
                    
                # Default to true as it seems common
                cmd.append("--node_anchored")
                    
                if config.get('visualize_instances', False):
                    cmd.append("--visualize_instances")
            
            print("Running command: {}".format(' '.join(cmd)), flush=True)
            print("Mining started - this may take several minutes...", flush=True)
            print("Job ID: {}".format(job_id), flush=True)
            
            # Use Popen to stream output in real-time
            import os as os_module
            env = os_module.environ.copy()
            env['PYTHONUNBUFFERED'] = '1'
            
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
                env=env
            )
            
            # Stream output line by line
            total_chunks = 1
            current_chunk = 0
            
            progress_file = os.path.join(shared_job_dir, "progress.json")
            
            def update_progress(status, progress, message):
                with open(progress_file, 'w') as f:
                    json.dump({
                        "status": status,
                        "progress": min(progress, 99), # Never hit 100 until fully done
                        "message": message
                    }, f)

            # Initialize progress
            update_progress("starting", 0, "Initializing miner...")

            for line in process.stdout:
                line_str = line.rstrip()
                print(line_str, flush=True)
                
                try:
                    # Robust parsing that ignores timestamp prefixes
                    # Example: "[10:00:00] Worker PID 123 finished chunk 1/4"
                    
                    if "started chunk" in line_str:
                         # "... started chunk 1/4"
                        parts = line_str.split("started chunk")[1].strip().split(" ")[0] # "1/4"
                        current, total = map(int, parts.split("/"))
                        total_chunks = total
                        current_chunk = current
                        
                        # Start of a chunk is roughly (chunk-1)/total
                        base_progress = int(((current_chunk - 1) / total_chunks) * 90)
                        update_progress("mining", base_progress, f"Started processing chunk {current_chunk} of {total_chunks}...")

                    elif "still processing chunk" in line_str:
                        # Bump progress slightly to show activity
                        # "... still processing chunk 1/4"
                        parts = line_str.split("still processing chunk")[1].strip().split(" ")[0]
                        current, total = map(int, parts.split("/"))
                        
                        base_progress = int(((current_chunk - 1) / total_chunks) * 90)
                        active_progress = base_progress + int((1 / total_chunks) * 45) # Halfway through chunk
                        update_progress("mining", active_progress, f"Still processing chunk {current_chunk} of {total_chunks}...")

                    elif "finished chunk" in line_str:
                        # "... finished chunk 1/4"
                        parts = line_str.split("finished chunk")[1].strip().split(" ")[0]
                        current, total = map(int, parts.split("/"))
                        
                        # End of chunk is current/total
                        completed_progress = int((current_chunk / total_chunks) * 90)
                        update_progress("mining", completed_progress, f"Finished chunk {current_chunk} of {total_chunks}")
                        
                except Exception as e:
                    # Don't let parsing errors stop the stream
                    print(f"Warning: Failed to parse progress line: {e}", flush=True)

            process.wait()
            
            # Final completion update
            update_progress("completed", 100, "Mining completed successfully!")
            
            if process.returncode != 0:
                raise Exception("Miner failed with exit code {}".format(process.returncode))

            # Read the results
            if not os.path.exists(json_path):
                 raise Exception('Result file not found')

            with open(json_path, 'r') as f:
                mining_results = json.load(f)

            shared_results_dir = os.path.join(shared_job_dir, "results")
            shared_plots_dir = os.path.join(shared_job_dir, "plots")
            os.makedirs(shared_results_dir, exist_ok=True)
            os.makedirs(shared_plots_dir, exist_ok=True)
            
            # Persistent folders in submodule root (Standard location for LLM and local access)
            persistent_results_dir = Config.RESULTS_FOLDER
            persistent_plots_dir = os.path.join(Config.BASE_DIR, "plots", "cluster")
            os.makedirs(persistent_results_dir, exist_ok=True)
            os.makedirs(persistent_plots_dir, exist_ok=True)

            # 1. Handle Basic Pattern Results
            if os.path.exists(out_path):
                # Copy to shared results for download
                shutil.copy(out_path, os.path.join(shared_results_dir, "patterns.pkl"))
                # Copy to persistent results for latest job context (fixed name)
                shutil.copy(out_path, os.path.join(persistent_results_dir, "patterns.pkl"))

            if os.path.exists(json_path):
                # Copy to shared results for download
                shutil.copy(json_path, os.path.join(shared_results_dir, "patterns.json"))

            # 2. Handle Instance Files (when visualize_instances=True)
            if os.path.exists(instance_pkl_path):
                # Copy to shared results for download
                shutil.copy(instance_pkl_path, os.path.join(shared_results_dir, "patterns_all_instances.pkl"))
                print(f"Copied instance PKL file to shared results", flush=True)

            if os.path.exists(instance_json_path):
                # Copy to shared results for download
                shutil.copy(instance_json_path, os.path.join(shared_results_dir, "patterns_all_instances.json"))
                print(f"Copied instance JSON file to shared results", flush=True)

            # 3. Handle Plot Files and Directories
            plots_cluster_dir = "/app/plots/cluster"
            if os.path.exists(plots_cluster_dir):
                # Create cluster subdirectory in shared plots
                shared_cluster_dir = os.path.join(shared_plots_dir, "cluster")
                os.makedirs(shared_cluster_dir, exist_ok=True)

                for filename in os.listdir(plots_cluster_dir):
                    src_path = os.path.join(plots_cluster_dir, filename)
                    dst_path = os.path.join(shared_cluster_dir, filename)

                    if os.path.isfile(src_path):
                        # Copy individual files (representative mode PNG/PDF)
                        shutil.copy(src_path, dst_path)
                    elif os.path.isdir(src_path):
                        # Copy directories (instance mode HTML folders)
                        if os.path.exists(dst_path):
                            shutil.rmtree(dst_path)
                        shutil.copytree(src_path, dst_path)
                        print(f"Copied instance plot directory: {filename}", flush=True)
            
            print("Results saved to shared volume: {}".format(shared_job_dir), flush=True)
            
            return {
                "motifs": mining_results,
                "job_id": job_id,
                "results_path": "/shared/output/{}/results".format(job_id),
                "plots_path": "/shared/output/{}/plots".format(job_id)
            }

        finally:
            # Cleanup temporary output files
            # Note: We need to clean up instance files too
            if os.path.exists(out_path):
                os.remove(out_path)
            if os.path.exists(json_path):
                os.remove(json_path)
            if os.path.exists(instance_json_path):
                os.remove(instance_json_path)
            if os.path.exists(instance_pkl_path):
                os.remove(instance_pkl_path)